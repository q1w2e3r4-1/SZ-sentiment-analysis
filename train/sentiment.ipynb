{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "651c24a7-45f0-44b5-8c29-66bec63eb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# df = pd.read_csv(\"news.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ee61c-cbd9-422a-959b-16d5612b1d7e",
   "metadata": {},
   "source": [
    "# 训练部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a83bfa2-3c16-46cc-9541-ace349119b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>人民创投携手游族网络、中科曙光揭牌成立 “新质生产力数字化创新联盟” 5月11日下午,由人民...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航天动力(600343.SH)及高管因虚假信息披露遭上交所处分 2024年4月2日，上海证券...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>中国重工：2023年度净利润约-7.82亿元 中国重工（SH601989，收盘价：4.94元...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>鲁 泰Ａ：公司产业链较长 各生产环节均存在期末在产品 每经AI快讯，有投资者在投资者互动平台...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>方正电机：2024年第一季度净利润约-834万元 方正电机（SZ002196，收盘价：5.5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y\n",
       "0  人民创投携手游族网络、中科曙光揭牌成立 “新质生产力数字化创新联盟” 5月11日下午,由人民...  1\n",
       "1  航天动力(600343.SH)及高管因虚假信息披露遭上交所处分 2024年4月2日，上海证券...  0\n",
       "2  中国重工：2023年度净利润约-7.82亿元 中国重工（SH601989，收盘价：4.94元...  0\n",
       "3  鲁 泰Ａ：公司产业链较长 各生产环节均存在期末在产品 每经AI快讯，有投资者在投资者互动平台...  0\n",
       "4  方正电机：2024年第一季度净利润约-834万元 方正电机（SZ002196，收盘价：5.5...  0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import random\n",
    "#\n",
    "#df = df[['title','content']]\n",
    "#df[\"content\"] = df[\"content\"].apply(lambda x: x.strip('{').strip('}'))\n",
    "#df['text'] = df['title'] + \" \" + df['content']\n",
    "#df['y'] = np.random.randint(0, 2, size=len(df))\n",
    "#df = df.sample(frac=1).reset_index(drop=True)\n",
    "#df_raw = df[['text', 'y']]\n",
    "#\n",
    "#df_raw = df_raw[0:400]\n",
    "#df_raw.to_csv(\"haha.csv\")\n",
    "#df_raw.head(20)\n",
    "\n",
    "df = pd.read_csv(\"./train_data/train.csv\")\n",
    "df_raw = df[['text','y']]\n",
    "df_raw = df_raw[df_raw['y'] != 0]\n",
    "df_raw.loc[df_raw['y'] == -1, 'y'] = 0\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1a37b763-44d5-4bbb-aa6e-63aec6e521fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at /home/aiia/newDisk/q1w2e3r4/sentimics and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "MODEL_PATH = r\"/home/aiia/newDisk/q1w2e3r4/sentimics\"\n",
    "\n",
    "num_classes = 2 # only pos and neg\n",
    "# a.通过词典导入分词器\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(r\"/home/aiia/newDisk/q1w2e3r4/sentimics/vocab.txt\") \n",
    "# b. 导入配置文件\n",
    "model_config = transformers.BertConfig.from_pretrained(MODEL_PATH)\n",
    "# 修改配置\n",
    "model_config.output_hidden_states = True\n",
    "model_config.output_attentions = True\n",
    "model_config.num_labels = num_classes\n",
    "# 通过配置和路径导入模型\n",
    "model = transformers.TFBertForSequenceClassification.from_pretrained(MODEL_PATH,config = model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "52fb7f32-1476-437c-a83f-fd8966eb53bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['[CLS]', '人', '民', '创', '投', '携', '手', '游', '族', '网', '络', '、', '中', '科', '曙', '光', '揭', '牌', '成', '立', '[UNK]', '新', '质', '生', '产', '力', '数', '字', '化', '创', '新', '联', '盟', '[UNK]', '5', '月', '11', '日', '下', '午', ',', '由', '人', '民', '网', '人', '民', '创', '投', '、', '中', '科', '曙', '光', '、', '游', '族', '网', '络', '共', '同', '发', '起', '的', '新', '质', '生', '产', '力', '数', '字', '化', '创', '新', '联', '盟', '正', '式', '成', '立', '并', '举', '行', '揭', '牌', '仪', '式', '。', ';', '联', '盟', '相', '关', '负', '责', '人', '表', '示', ',', '随', '着', '人', '工', '智', '能', '等', '新', '一', '代', '信', '息', '技', '术', '的', '高', '速', '发', '展', ',', '由', '技', '术', '革', '命', '性', '突', '破', '、', '生', '产', '要', '素', '创', '新', '性', '配', '置', '、', '产', '业', '转', '型', '升', '级', '而', '催', '生', '的', '先', '进', '生', '产', '力', '质', '态', '加', '速', '形', '成', ',', '代', '表', '先', '进', '生', '产', '力', '演', '进', '方', '向', '的', '新', '质', '生', '产', '力', '应', '运', '而', '生', '。', '为', '建', '立', '一', '个', '促', '进', '产', '业', '界', '、', '学', '术', '界', '、', '投', '资', '界', '交', '流', '与', '合', '作', '的', '开', '放', '平', '台', ',', '推', '进', '数', '字', '技', '术', '创', '新', '与', '融', '合', ',', '赋', '能', '企', '业', '数', '字', '化', '转', '型', ',', '同', '时', '助', '力', '相', '关', '政', '策', '与', '行', '业', '标', '准', '制', '定', ',', '三', '家', '发', '起', '单', '位', '决', '定', '依', '托', '各', '自', '优', '势', ',', '共', '同', '发', '起', '并', '推', '动', '建', '设', '新', '质', '生', '产', '力', '数', '字', '化', '创', '新', '联', '盟', ',', '通', '过', '整', '合', '优', '质', '企', '业', '、', '科', '研', '院', '所', '、', '投', '资', '机', '构', '等', '各', '方', '力', '量', ',', '逐', '步', '在', '数', '字', '经', '济', '领', '域', '形', '成', '一', '个', '紧', '密', '连', '接', '、', '高', '效', '协', '同', '、', '有', '机', '统', '一', '的', '创', '新', '生', '态', ',', '从', '而', '为', '我', '国', '新', '质', '生', '产', '力', '的', '进', '一', '步', '发', '展', '作', '出', '贡', '献', '。', ';', '中', '科', '曙', '光', '作', '为', '中', '科', '院', '旗', '下', '控', '股', '企', '业', ',', '在', '超', '算', '、', '存', '储', '、', '安', '全', '、', '数', '据', '中', '心', '等', '领', '域', '[SEP]']\n",
      "{'token_ids': [101, 782, 3696, 1158, 2832, 3025, 2797, 3952, 3184, 5381, 5317, 510, 704, 4906, 3282, 1045, 2999, 4277, 2768, 4989, 100, 3173, 6574, 4495, 772, 1213, 3144, 2099, 1265, 1158, 3173, 5468, 4673, 100, 126, 3299, 8111, 3189, 678, 1286, 117, 4507, 782, 3696, 5381, 782, 3696, 1158, 2832, 510, 704, 4906, 3282, 1045, 510, 3952, 3184, 5381, 5317, 1066, 1398, 1355, 6629, 4638, 3173, 6574, 4495, 772, 1213, 3144, 2099, 1265, 1158, 3173, 5468, 4673, 3633, 2466, 2768, 4989, 2400, 715, 6121, 2999, 4277, 811, 2466, 511, 132, 5468, 4673, 4685, 1068, 6566, 6569, 782, 6134, 4850, 117, 7390, 4708, 782, 2339, 3255, 5543, 5023, 3173, 671, 807, 928, 2622, 2825, 3318, 4638, 7770, 6862, 1355, 2245, 117, 4507, 2825, 3318, 7484, 1462, 2595, 4960, 4788, 510, 4495, 772, 6206, 5162, 1158, 3173, 2595, 6981, 5390, 510, 772, 689, 6760, 1798, 1285, 5277, 5445, 998, 4495, 4638, 1044, 6822, 4495, 772, 1213, 6574, 2578, 1217, 6862, 2501, 2768, 117, 807, 6134, 1044, 6822, 4495, 772, 1213, 4028, 6822, 3175, 1403, 4638, 3173, 6574, 4495, 772, 1213, 2418, 6817, 5445, 4495, 511, 711, 2456, 4989, 671, 702, 914, 6822, 772, 689, 4518, 510, 2110, 3318, 4518, 510, 2832, 6598, 4518, 769, 3837, 680, 1394, 868, 4638, 2458, 3123, 2398, 1378, 117, 2972, 6822, 3144, 2099, 2825, 3318, 1158, 3173, 680, 6084, 1394, 117, 6602, 5543, 821, 689, 3144, 2099, 1265, 6760, 1798, 117, 1398, 3198, 1221, 1213, 4685, 1068, 3124, 5032, 680, 6121, 689, 3403, 1114, 1169, 2137, 117, 676, 2157, 1355, 6629, 1296, 855, 1104, 2137, 898, 2805, 1392, 5632, 831, 1232, 117, 1066, 1398, 1355, 6629, 2400, 2972, 1220, 2456, 6392, 3173, 6574, 4495, 772, 1213, 3144, 2099, 1265, 1158, 3173, 5468, 4673, 117, 6858, 6814, 3146, 1394, 831, 6574, 821, 689, 510, 4906, 4777, 7368, 2792, 510, 2832, 6598, 3322, 3354, 5023, 1392, 3175, 1213, 7030, 117, 6852, 3635, 1762, 3144, 2099, 5307, 3845, 7566, 1818, 2501, 2768, 671, 702, 5165, 2166, 6825, 2970, 510, 7770, 3126, 1291, 1398, 510, 3300, 3322, 5320, 671, 4638, 1158, 3173, 4495, 2578, 117, 794, 5445, 711, 2769, 1744, 3173, 6574, 4495, 772, 1213, 4638, 6822, 671, 3635, 1355, 2245, 868, 1139, 6567, 4346, 511, 132, 704, 4906, 3282, 1045, 868, 711, 704, 4906, 7368, 3186, 678, 2971, 5500, 821, 689, 117, 1762, 6631, 5050, 510, 2100, 996, 510, 2128, 1059, 510, 3144, 2945, 704, 2552, 5023, 7566, 1818, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "max_length_test = 400\n",
    "\n",
    "test_sentence_with_special_tokens = '[CLS]' + df['text'][0][:max_length_test] + '[SEP]'\n",
    "tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)\n",
    "print('tokenized', tokenized)\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "\n",
    "padding_length = max_length_test - len(input_ids)\n",
    "\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "\n",
    "token_type_ids = [0] * max_length_test\n",
    "bert_input = {\n",
    "    \"token_ids\": input_ids,\n",
    "    \"token_type_ids\": token_type_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "} \n",
    "\n",
    "print(bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24ee908c-6399-4381-86a5-2dba85b6bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def convert_example_to_feature(review):\n",
    "    return tokenizer.encode_plus(review, \n",
    "                add_special_tokens = True, \n",
    "                max_length = max_length, \n",
    "                pad_to_max_length = True, \n",
    "                return_attention_mask = True, \n",
    "                truncation=True)\n",
    "\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "\n",
    "    for index, row in ds.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        label = row[\"y\"]\n",
    "        bert_input = convert_example_to_feature(review)\n",
    "\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "def split_dataset(df):\n",
    "    train_set, x = train_test_split(df, \n",
    "        stratify=df['y'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "    val_set, test_set = train_test_split(x, \n",
    "        stratify=x['y'],\n",
    "        test_size=0.5, \n",
    "        random_state=43)\n",
    "\n",
    "    return train_set,val_set, test_set\n",
    "\n",
    "max_length = 400\n",
    "batch_size = 128\n",
    "\n",
    "# split data\n",
    "train_data,val_data, test_data = split_dataset(df_raw)\n",
    "\n",
    "# train dataset\n",
    "ds_train_encoded = encode_examples(train_data).batch(batch_size)\n",
    "# val dataset\n",
    "ds_val_encoded = encode_examples(val_data).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(test_data).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "138ab413-f377-4dd7-985f-1eb1b05882c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  102267648 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102269186 (390.13 MB)\n",
      "Trainable params: 102269186 (390.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5\n",
    "number_of_epochs = 8\n",
    "\n",
    "# 三件套\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7618e0db-0152-495e-82f0-137db17d5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "3/3 [==============================] - 91s 18s/step - loss: 0.7034 - accuracy: 0.5353 - val_loss: 0.5418 - val_accuracy: 0.7333\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 55s 15s/step - loss: 0.5953 - accuracy: 0.6989 - val_loss: 0.5199 - val_accuracy: 0.7333\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 56s 15s/step - loss: 0.5477 - accuracy: 0.7100 - val_loss: 0.5098 - val_accuracy: 0.8000\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 58s 15s/step - loss: 0.5094 - accuracy: 0.8067 - val_loss: 0.4576 - val_accuracy: 0.8667\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 58s 15s/step - loss: 0.4401 - accuracy: 0.8550 - val_loss: 0.3979 - val_accuracy: 0.8667\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 57s 15s/step - loss: 0.3809 - accuracy: 0.8625 - val_loss: 0.3215 - val_accuracy: 0.8667\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 58s 15s/step - loss: 0.3035 - accuracy: 0.9071 - val_loss: 0.2420 - val_accuracy: 0.9333\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 57s 15s/step - loss: 0.2388 - accuracy: 0.9331 - val_loss: 0.1815 - val_accuracy: 0.9333\n",
      "1/1 [==============================] - 1s 918ms/step - loss: 0.1241 - accuracy: 1.0000\n",
      "# evaluate test_set: [0.12408402562141418, 1.0]\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_val_encoded)\n",
    "# evaluate test_set\n",
    "print(\"# evaluate test_set:\",model.evaluate(ds_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "284e8fb3-ebd2-408e-8874-92d92ce452ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2769, 1599, 3614, 5273, 5682, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"我喜欢红色\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "08f8f6c2-5740-4dbd-aab1-1f55886d396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  102267648 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102269186 (390.13 MB)\n",
      "Trainable params: 102269186 (390.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31839254-2545-4c49-a7e4-a83134439d36",
   "metadata": {},
   "source": [
    "# 预测部分，如果使用加载模型请从这里开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "03bd8ac5-95b7-411d-8c9b-8b4f7af0a345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  102267648 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102269186 (390.13 MB)\n",
      "Trainable params: 102269186 (390.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的模型\n",
    "'''\n",
    "model = tf.keras.models.load_model(\"my_model\")\n",
    "\n",
    "num_classes = 2\n",
    "model_config = transformers.BertConfig.from_pretrained(MODEL_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "model_config.output_attentions = True\n",
    "model_config.num_labels = num_classes\n",
    "\n",
    "model.config = model_config\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3b156044-9773-46b9-a8d3-75bdbee31bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'can_generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# AutoTokenizer用于tokenize，可以通俗理解为分词\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/q1w2e3r4/lib/python3.9/site-packages/transformers/pipelines/__init__.py:1108\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/q1w2e3r4/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:83\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     86\u001b[0m         TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m     89\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/q1w2e3r4/lib/python3.9/site-packages/transformers/pipelines/base.py:896\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtask_specific_params\u001b[38;5;241m.\u001b[39mget(task))\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Pipelines calling `generate`: if the tokenizer has a pad token but the model doesn't, set it in the\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# forward params so that `generate` is aware of the pad token.\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcan_generate\u001b[49m()\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    899\u001b[0m ):\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'can_generate'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# AutoTokenizer用于tokenize，可以通俗理解为分词\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4813b0-9640-49f4-a813-fd092c85f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyze(text):    \n",
    "    scores = []\n",
    "    sub_len = len(text)//400+1\n",
    "    for i in range(sub_len):\n",
    "        sub_text = text[i*400:(i+1)*400]\n",
    "        res = classifier(sub_text)[0]\n",
    "        s = res['score']\n",
    "        if res['label'] == 'LABEL_0':  # neg\n",
    "            s = -s\n",
    "        scores.append(s)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9fc1a-7f1f-47e4-a5e6-08ca6a7fc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = ''' 人民创投携手游族网络、中科曙光揭牌成立 “新质生产力数字化创新联盟” '''\n",
    "#text = '''不好看，电影院里睡了三个小时'''\n",
    "#text = '''航天动力(600343.SH)及高管因虚假信息披露遭上交所处分'''\n",
    "text = '''2024年以来，启迪环境已连发4次关于其全资子公司、子公司收到《行政处罚决定书》的公告'''\n",
    "\n",
    "sentiment_analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014faa35-d498-43b9-a93f-a6ae6b0d4924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076fc45-25a8-4ea1-9d80-65fbff0bebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we test it\n",
    "df = pd.read_csv(\"news.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdd3ad-8d8b-4412-aef7-b1238582a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = df[\"content\"].apply(lambda x: x.strip('{').strip('}'))\n",
    "df['text'] = df['title'] + df['content']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f3841-8bb3-4032-a14a-9100ac5e481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LEN = 100\n",
    "def is_chinese(uchar):\n",
    "    # 判断当前字符是否为中文字符\n",
    "    return uchar >= u'\\u4e00' and uchar <= u'\\u9fa5'\n",
    "    \n",
    "def strlen(s):\n",
    "    ret = 0\n",
    "    for c in s:\n",
    "        if is_chinese(c):\n",
    "            ret += 2\n",
    "        else:\n",
    "            ret += 1\n",
    "    return ret\n",
    "    \n",
    "def ljust(s):\n",
    "    l = strlen(s)\n",
    "    padding = max(0, FIXED_LEN - l)\n",
    "    return s + ' ' * padding\n",
    "    \n",
    "for index,row in df.iterrows():\n",
    "    text = row['text']\n",
    "    print(\"%s\\t%f\"%(ljust(row['title']), sentiment_analyze(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "24096b70-628f-4fc7-af7e-7e65abe3adcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "62dcf352-a067-45dd-ad35-5ccb1830b172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cf03bb55-3c47-4d0b-814a-a7d5a4a5d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  102267648 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102269186 (390.13 MB)\n",
      "Trainable params: 102269186 (390.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9d41f-68c0-422f-b409-afab0e00cf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
